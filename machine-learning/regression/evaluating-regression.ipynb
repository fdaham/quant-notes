{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Regression\n",
    "\n",
    "## Error Metrics\n",
    "\n",
    "Error metrics, or residuals, are used in linear regression models to evaluate performance. They measure the difference between the actual and predicted model values. \n",
    "\n",
    "### Mean Absolute Error\n",
    "\n",
    "Mean Absolute Error (MAE) is the average absolute difference between actual and predicted model values. \n",
    "\n",
    "\\begin{equation*}      \n",
    "\\text{MAE} = \\sum_{i=1}^{n} \\frac{|y_i - x_i|}{n}\n",
    "\\label{eq:1} \\tag{1}\n",
    "\\end{equation*} \n",
    "\n",
    "MAE handles outliers well because of the absolute value and large errors don't overpower the smaller ones, so its output gives an unbiased view of the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.575"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_actual = [3, -0.5, 1, 3.5]\n",
    "y_pred = [2.5, 0, 0.2, 3]\n",
    "mean_absolute_error(y_actual, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "Mean Squared Error (MSE) is the average squared difference between actual and predicted model values. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2 \n",
    "\\label{eq:2} \\tag{2}\n",
    "\\end{equation*} \n",
    "\n",
    "Unlike MAE, MSE heavily counts against outliers. The square allows larger errors to overpower the smaller ones, which means a single outlier could yield a high error value--allowing us to assume that the model is worse than it actually is. On the other hand, if all the errors are small (i.e. less than 1), we may assume the model is better than it actually is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34750000000000003"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_actual, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error\n",
    "\n",
    "Root Mean Squared Error (RMSE) is the average root-squared difference between actual and predicted model values. It's the root-squared value of MSE.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2} \n",
    "\\label{eq:3} \\tag{3}\n",
    "\\end{equation*}\n",
    "\n",
    "RMSE is the easiest statistic to interpret because it's measured in the same units as what we're trying to predict. For example, if we were to predict rent prices using linear regression, RMSE would show the error of our model in dollars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5894913061275798"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_actual, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Squared\n",
    "\n",
    "The $R^2$ score, or the coefficient of determination, measures the strength of the relationship between independent and dependent variables $X$ and $Y$. The closer it is to 1, the better. $R^2$ is the square of the correlation coefficient, $R$: \n",
    "\n",
    "\\begin{equation*}   \n",
    "\\text{R} = \\frac{\\sum (x - \\bar{x})(y - \\bar{y})}{\\sqrt{\\sum (x - \\bar{x})^2} \\sqrt{\\sum (y - \\bar{y})^2}}\n",
    "\\label{eq:4} \\tag{4}\n",
    "\\end{equation*} \n",
    "\n",
    "Below is the equation for $R^2$, where $\\text{SSE}$ is the sum of squared errors and $\\text{SST}$ is the total sum of squares.\n",
    "\n",
    "\\begin{equation*}   \n",
    "\\text{R}^2 = 1 - \\frac{\\sum_{i=1}^m (y_i - \\hat{y_i})^2}{\\sum_{i=1}^m (y_i - \\bar{y})^2} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "\\label{eq:5} \\tag{5}\n",
    "\\end{equation*} \n",
    "\n",
    "The **Adjusted $R^2$** score is a modified version of $R^2$ that accounts for the number of predictors in the model. It increases if a new term improves the fit and decreases if there's little to no improvement. Below is the equation for $\\text{R}_{\\text{adj}}^2$, where $n$ is the total sample size and $k$ is the number of predictors.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{R}_{\\text{adj}}^2 =  1 - \\frac{\\text{SSE}}{\\text{SST}} \\cdot \\frac{n-1}{n-k-1} \n",
    "\\label{eq:6} \\tag{6}\n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_actual, y_pred)\n",
    "n, k = len(y_actual), x_pred.shape[1]\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivation\n",
    "\n",
    "The correlation coefficient $R$ can be calculated as the cosine of the angle $\\theta$ between two vectors $u$ and $v$, where $u$ and $v$ are the normalized vectors of $x$ and $y$ (i.e. $u = x - \\bar{x}$ and $v = y - \\bar{y}$). We can use the [definition of the dot product](http://sites.science.oregonstate.edu/math/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/dotprod/dotprod.html) to find $\\text{cos}(\\theta)$:\n",
    "\n",
    "\\begin{equation*}   \n",
    "\\begin{split}\n",
    "\\text{cos}(\\theta) &= \\frac{u \\cdot v}{\\lVert u \\rVert \\lVert v \\rVert} \n",
    "= \\frac{u^T v}{\\sqrt{u^T u}\\sqrt{v^T v}} \\\\\n",
    "&= \\frac{u_1v_1 + \\cdots + u_nv_n}{\\sqrt{u_1^2 + \\cdots + u_n^2}\\sqrt{v_1^2 + \\cdots + v_n^2}} \\\\\n",
    "&= \\frac{\\sum uv}{\\sqrt{\\sum u^2} \\sqrt{\\sum v^2}}\n",
    "= \\frac{\\sum (x - \\bar{x})(y - \\bar{y})}{\\sqrt{\\sum (x - \\bar{x})^2} \\sqrt{\\sum (y - \\bar{y})^2}} = R\n",
    "\\end{split}\n",
    "\\label{eq:7} \\tag{7}\n",
    "\\end{equation*} \n",
    "\n",
    "#### Example\n",
    "\n",
    "*Running an OLS regression of $y$ ~ $x_1$ gives $R_1^2 = 0.3$. Running an OLS regression of $y$ ~ $x_2$ gives $R_2^2 = 0.4$. Find $R^2$ for $y$ ~ $x_1 + x_2$.* \n",
    "\n",
    "$R_1^2 \\neq R_2^2$, which means $x_1 \\neq x_2$. We don't know anything else about $x_1$ and $x_2$, so we'll need to express $R^2$ as a range of values. The maximum bound for $R^2$ is reached when $x_1$ and $x_2$ are orthogonal, or uncorrelated. In this case, $R^2$ is at most equal to $1$. This is easy to explain with geometry. \n",
    "\n",
    "<img src=\"Images/geometric_ols.jpg\" alt=\"img1\" width=\"300\"/>\n",
    "\n",
    "$R$ is the cosine of the angle $\\theta$ between vectors $y$ and $x_1 + x_2$. At best, $\\theta=0$ and $y$ exists in the column space of $x$, which means $y$ can be written as a function of $x_1 + x_2$ and $R^2 = 1$. Alternatively, the minimum bound for $R^2$ is reached when $x_2$ contains all the information of $x_1$. In this case, $R^2 = R_2^2 = 0.4$. Therefore, $0.4 \\leq R^2 \\leq 1$. \n",
    "\n",
    "This result is simulated below using the following equation for $R^2$, written in terms of correlation coefficients:\n",
    "\n",
    "\\begin{equation*}   \n",
    "\\text{R}^2 = \\frac{r_{y,x_1}^2 + r_{y,x_2}^2 - 2 r_{y,x_1}r_{y,x_2}r_{x_1,x_2}}{1 - r_{x_1,x_2}^2}\n",
    "\\label{eq:8} \\tag{8}\n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "r2_y1, r2_y2 = 0.3, 0.4\n",
    "r2_12 = np.array(range(100))/100\n",
    "y = (r2_y1 + r2_y2 - 2*(r2_y1**0.5)*(r2_y2**0.5)*(r2_12**0.5))/(1-r2_12)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('$R^2$')\n",
    "plt.xlabel('$r_{x_1,x_2}^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis\n",
    "\n",
    "### Coefficients\n",
    "\n",
    "Coefficients describe the relationship between each independent variable and the dependent variable. The first coefficient without an input is called the **intercept**. Think of it as the base case of your model. The intercept adjusts what the model predicts when all your inputs are 0. \n",
    "\n",
    "The remaining coefficients are the **regression coefficients**. They give the slope of the line of best fit. The sign of a regression coefficient tells you whether there is a positive or negative correlation between each independent variable and the dependent variable. A positive coefficient indicates that the mean of the dependent variable will increase as the value of the independent variable increases. A negative coefficient indicates that the dependent variable will decrease as the independent variable increases.\n",
    "\n",
    "For a simple linear regression, the regression coefficient can be written in terms of the correlation coefficient and standard deviations of $x$ and $y$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\beta = r_{yx} \\frac{\\sigma_y}{\\sigma_x}\n",
    "\\label{eq:9} \\tag{9}\n",
    "\\end{equation*}  \n",
    "\n",
    "This can be derived geometrically. Imagine two vectors $y$ and $x$ with an angle $\\theta$ between them. The length of each vector is their respective standard deviation and $\\text{cos}(\\theta)$ is the correlation between $y$ and $x$. We know that the projection of $y$ onto $x$ gives us $\\hat{y} = \\beta x = y \\text{cos}(\\theta)$ which we can rewrite as $\\beta \\sigma_x = \\sigma_y r_{yx} \\Rightarrow \\beta = r_{yx} \\frac{\\sigma_y}{\\sigma_x}$.   \n",
    "\n",
    "A regression with two independent variables, the regression coefficients $\\beta_1$ and $\\beta_2$ are calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\beta_1 &= \\frac{\\sum x_2^2 \\sum x_1 y - \\sum x_1 x_2 \\sum x_2 y}{\\sum x_1^2 \\sum x_2^2 - \\sum x_1 x_2} \\\\\n",
    "\\beta_2 &= \\frac{\\sum x_1^2 \\sum x_2 y - \\sum x_1 x_2 \\sum x_1 y}{\\sum x_1^2 \\sum x_2^2 - \\sum x_1 x_2}\n",
    "\\end{split}\n",
    "\\label{eq:10} \\tag{10}\n",
    "\\end{equation*}   \n",
    "\n",
    "We can also find $R^2$ using these regression coefficients:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{R}^2 = \\beta_1 r_{y,x_1} + \\beta_2 r_{y,x_2}\n",
    "\\label{eq:11} \\tag{11}\n",
    "\\end{equation*}  \n",
    "\n",
    "### Statistical Testing\n",
    "\n",
    "See [here](http://mathcenter.oxford.emory.edu/site/math117/hypothesisTestsProportionsOneSample/) for an in-depth explanation (with an example) of hypothesis testing, standard errors, t-statistics, p-values, and the empirical rule. To summarize, let's say we have the following null and alternate hypothesis:\n",
    "\n",
    "\\begin{equation*}   \n",
    "\\text{H}_0: p = 0.5, \\text{H}_1: p \\neq 0.5 \n",
    "\\end{equation*} \n",
    "\n",
    "The following can be used to find the **standard error**:\n",
    "\n",
    "\\begin{equation*}   \n",
    "\\text{SE} = \\frac{\\sigma}{\\sqrt{n}} = \\sqrt{\\frac{p(1-p)}{n}}\n",
    "\\label{eq:12} \\tag{12}\n",
    "\\end{equation*} \n",
    "\n",
    "where $\\sigma$ is the sample standard deviation of a binomial distribution and $n$ is the sample size. From here, we can calculate the **t-statistic**, or the z-score of the sample distribution:\n",
    "\n",
    "\\begin{equation*}   \n",
    "t = \\frac{x - \\mu}{\\text{SE}}\n",
    "\\label{eq:13} \\tag{13}\n",
    "\\end{equation*} \n",
    "\n",
    "where $x$ is the observed $p$ and $\\mu = p$ for a normal distribution. You can use this $t$ to find the p-value, or the probability that $X$ falls after $t$. If the p-value is less than the significance level, $\\alpha$, we reject the null hypothesis. You can also calculate $t$ in terms of the correlation coefficient:\n",
    "\n",
    "\\begin{equation*}   \n",
    "t = r \\cdot \\frac{n-2}{1-r^2}\n",
    "\\label{eq:14} \\tag{14}\n",
    "\\end{equation*} \n",
    "\n",
    "Instead of p-values, the **empirical rule** can be used to assess the null hypothesis. The rule states that 68.3% of the probability distribution falls within one standard deviation of the mean, 95.5% within two, and 99.7% within three. For example, if $t = 2.2$ and $\\alpha = 0.05$, I can discount at least 95.5% of my probability distribution--leaving only 4.5%. 0.045 is less than $\\alpha$, so I'd reject my null hypothesis. \n",
    "\n",
    "#### Example\n",
    "\n",
    "*A coin lands on heads 55% of the time after $n$ tosses. How big should $n$ be in order to determine that the coin is biased? Before calculating $n$, give an intuitive estimate.*\n",
    "\n",
    "First, how large would you guess $n$ is? It has to be really large since the observed sample probability of $0.55$ is very close to that of an unbiased coin. The more coin tosses we get, the more confident we'll be in our hypothesis. So, I would guess several hundred.   \n",
    "\n",
    "Let's now calculate $n$. Our null hypothesis is $H_0: p = 0.5$. Our standard error is then: \n",
    "\n",
    "\\begin{equation*}   \n",
    "\\text{SE} = \\sqrt{\\frac{p(1-p)}{n}} = \\frac{0.5}{\\sqrt{n}}\n",
    "\\end{equation*} \n",
    "\n",
    "which gives the following confidence interval:\n",
    "\n",
    "\\begin{equation*}   \n",
    "t = \\frac{x - p}{\\text{SE}} = \\frac{0.55 - 0.5}{\\frac{0.5}{\\sqrt{n}}} = 0.1\\sqrt{n}\n",
    "\\end{equation*} \n",
    "\n",
    "Using the empirical rule and the de facto significance level of $\\alpha = 0.05$, we can reject our null hypothesis if $t \\geq 2$ since only 4.5% (less than $\\alpha$) of our normal distribution falls outside of two standard deviations from the mean. So,\n",
    "\n",
    "\\begin{equation*}   \n",
    "\\begin{split}\n",
    "&0.1\\sqrt{n} = t \\geq 2 \\\\\n",
    "&\\Rightarrow n \\geq \\left( \\frac{2}{0.1} \\right)^2 = 400\n",
    "\\end{split}\n",
    "\\end{equation*} \n",
    "\n",
    "Therefore, assuming $\\alpha = 0.05$, $n$ has to be at least 400 in order to reject the null hypothesis.  \n",
    "\n",
    "### Effects of Input Changes\n",
    "\n",
    "The table below summarizes which of the above regression variables change once the input data changes for some predicted output $\\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_n X_n + \\epsilon$.\n",
    "\n",
    "Type of Change | Effect on Coefficients | Effect on T-Statistic | Effect on $R^2$\n",
    "--------------------------------------------|-----------------------------|---------------------------|---------------------------\n",
    "Swapping $X$ and $Y$ in a simple linear model |The slope of the regression line will be a rescaled version of the old one; $r_{xy} = r_{yx}$ since the correlation coefficient is symmetrical, so $\\beta_{yx} = r_{xy} \\frac{\\sigma_y}{\\sigma_x}$  and $\\beta_{xy} = r_{yx} \\frac{\\sigma_x}{\\sigma_y} = \\beta_{yx} \\frac{\\sigma_x^2}{\\sigma_y^2}$| None; the correlation coefficients are symmetrical | None; $R^2 = r_{xy}^2 = r_{yx}^2$\n",
    "Duplicating the covariates | None; Generally, mean and variance of the sample remain the same. Mathematically, $\\beta$ remains unchanged ([proof](https://stats.stackexchange.com/questions/216003/what-are-the-consequences-of-copying-a-data-set-for-ols)). | Increases; t-statistic is directly proportional to the sample size | None; regression coefficients and correlation coefficients remain the same\n",
    "Changing the sample size, $n$ | Coefficients aren't directly affected by the sample size, but they are affected by the sampling variation. Sampling variability decreases as the sample size increases. | Increases or decreases | Affected by the sampling variation\n",
    "Adding a new covariate, $X_i$ | All coefficients are jointly estimated, so a new variable will change the existing coefficients | Affected by the change in the joint variance of $Y$ | Increases; any new nonzero term will improve the fit and increase $R^2$\n",
    "Removing a covariate, $X_i$ |All coefficients are jointly estimated, so removing a variable will change the remaining coefficients| Affected by the change in the joint variance of $Y$ | Affected by the change in the regression coefficient and variance\n",
    "Changing units for a covariate, $X_i$ | Changes the unit of the corresponding coefficient, $\\beta_i$ | None | None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
