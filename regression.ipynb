{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Regression models are used to make predictions of unknown values based on existing ones. It does this by building a statistical relationship between unknown (dependent) variables and known (independent) features.\n",
    "\n",
    "## Evaluating Regression\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\text{Mean Squared Error (MSE)} &= \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2 \\\\\n",
    "\\text{Root Mean Squared Error (RMSE)} &= \\sqrt{\\text{MSE}} \\\\         \n",
    "\\text{Mean Absolute Error (MAE)} &= \\sum_{i=1}^{n} \\frac{|y_i - x_i|}{n} \\\\\n",
    "\\text{Residual Sum of Squares (RSS)} &= \\sum (y_i - \\hat{y_i})^2 \\\\ \n",
    "\\text{Total Sum of Squares (SST)} &= \\sum (y_i - \\bar{y})^2 \\\\        \n",
    "\\text{Regression Sum of Squares (SSR)} &= \\sum (\\hat{y_i} - \\bar{y})^2 \\\\\n",
    "\\text{Sum of Squares Error (SEE)} &= \\sum (\\hat{y_i} - y)^2 \\\\                         \n",
    "\\text{R-Squared (}R^2) &= \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "\\end{split}\n",
    "\\label{eq:1} \\tag{1}\n",
    "\\end{equation*}                           \n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "Also known as ordinary least squares (OLS) regression. When two variables $X$ (dependent) and $Y$ (independent) are linearly related, their relationship can be written in the form of a regression line:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{Y} = a + BX\n",
    "\\label{eq:2} \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "where $a$ and $B$ are constants. Constant $B$ is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "B = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = \\rho \\frac{\\sigma_y}{\\sigma_x} \n",
    "\\label{eq:3} \\tag{3}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\rho$, the Pearson correlation coefficient, is equal to: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\rho = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}\n",
    "\\label{eq:4} \\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "The second constant, $a$, is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "a = \\frac{\\sum y_i}{n} - B \\frac{\\sum x_i}{n} = \\bar{y} - B \\bar{x}\n",
    "\\label{eq:5} \\tag{5}\n",
    "\\end{equation*}\n",
    "\n",
    "### Derivation\n",
    "\n",
    "Let's first revisit what we know. We're given $n$ inputs and outputs $(x_i, y_i)$, so we can define our line of best fit as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y_i} = a + B x_i\n",
    "\\label{eq:6} \\tag{6}\n",
    "\\end{equation*}\n",
    "\n",
    "such that it minimizes the following cost function (RSS):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "S &= \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\\\\n",
    "&= \\sum_{i=1}^{n} (y_i - a - B x_i)^2\n",
    "\\end{split}\n",
    "\\label{eq:7} \\tag{7}\n",
    "\\end{equation*}\n",
    "\n",
    "Now, we want to solve for constants $a$ and $B$. To do this, we should minimize $S$ by setting its partial derivative with respect to both $a$ and $B$ to zero. Let's start with $a$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\frac{\\delta S}{\\delta a} \\sum_{i=1}^{n} (y_i - a - B x_i)^2 = 0 \\\\\n",
    "&\\Rightarrow -2 \\sum_{i=1}^{n} (y_i - a - B x_i) = 0\n",
    "\\end{split} \n",
    "\\label{eq:8} \\tag{8}\n",
    "\\end{equation*}\n",
    "\n",
    "Knowing that $\\sum_{i=1}^{n} a = na$ and after expanding the above, we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\Rightarrow \\sum_{i=1}^{n} y_i - na - B \\sum_{i=1}^{n} x_i = 0 \\\\\n",
    "&\\Rightarrow a = \\frac{\\sum_{i=1}^{n} y_i - B \\sum_{i=1}^{n} x_i}{n} \\\\\n",
    "&\\Rightarrow \\boxed{a = \\bar{y} - B \\bar{x}}\n",
    "\\end{split} \n",
    "\\label{eq:9} \\tag{9}\n",
    "\\end{equation*}\n",
    "\n",
    "Now solving for $B$, we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\frac{\\delta S}{\\delta B} \\sum_{i=1}^{n} (y_i - a - B x_i)^2 = 0 \\\\\n",
    "&\\Rightarrow 2 \\sum_{i=1}^{n} -x_i (y_i - a - B x_i) = 0 \\\\ \n",
    "\\end{split}\n",
    "\\label{eq:10} \\tag{10}\n",
    "\\end{equation*}\n",
    "\n",
    "which, when combined with our definition of $a$ above, gives:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\sum_{i=1}^{n} (\\bar{y} x_i - x_i y_i - B \\bar{x} x_i + B x_i^2) = 0 \\\\\n",
    "\\Rightarrow B &= \\frac{\\sum_{i=1}^{n} x_i(y_i - \\bar{y})}{\\sum_{i=1}^{n} x_i (x_i - \\bar{x})} \\\\\n",
    "&= \\boxed{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}}\n",
    "\\end{split}\n",
    "\\label{eq:11} \\tag{11}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression is similar to simple linear regression except instead of one independent variable, there are multiple. When $Y$ has a linear dependency on $m$ variables $x_1, x_2,..., x_m$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{Y} = a + b_1 x_1 + b_2 x_2 + ... + b_m x_m\n",
    "\\label{eq:12} \\tag{12}\n",
    "\\end{equation*}\n",
    "\n",
    "## LASSO Regression\n",
    "\n",
    "Linear regression models can be subject to overfitting. LASSO regression, also known as **L1 regularization** (a type of feature selection in machine learning), aims to fix this by eliminating features from the model. It does this by setting feature coefficients to zero using an L1 penalty equal to the absolute value of the magnitude of the coefficients. It particularly works well in cases where there are a small number of significant parameters and the others have coefficients close to zero. \n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Also known as **L2 regularization**. It's similar to LASSO regression except, instead of eliminating features entirely, their coefficients are minimized to some number close to zero by adding an L2 penalty equal to the square of the magnitude of coefficients. Since its not getting rid of any features--only minimizing their effect on the model--ridge regression isn't considered part of feature selection. It works particularly well when there are many significant parameters with close to the same value. \n",
    "\n",
    "## Stepwise Regression\n",
    "\n",
    "Starts with the simplest model, evaluates its performance, then adds another variable, evaluates its performance, then compares it to the previous model, etc. until the best performing model is found."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
