{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Regression models are used to make predictions of unknown values based on existing ones. It does this by building a statistical relationship between unknown (dependent) variables and known (independent) features.\n",
    "\n",
    "## Evaluating Regression\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\text{Mean Squared Error (MSE)} &= \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2 \\\\\n",
    "\\text{Root Mean Squared Error (RMSE)} &= \\sqrt{\\text{MSE}} \\\\         \n",
    "\\text{Mean Absolute Error (MAE)} &= \\sum_{i=1}^{n} \\frac{|y_i - x_i|}{n} \\\\\n",
    "\\text{Residual Sum of Squares (RSS)} &= \\sum (y_i - \\hat{y_i})^2 \\\\ \n",
    "\\text{Total Sum of Squares (SST)} &= \\sum (y_i - \\bar{y})^2 \\\\        \n",
    "\\text{Regression Sum of Squares (SSR)} &= \\sum (\\hat{y_i} - \\bar{y})^2 \\\\\n",
    "\\text{Sum of Squares Error (SEE)} &= \\sum (\\hat{y_i} - y)^2 \\\\                         \n",
    "\\text{R-Squared (}R^2) &= \\frac{\\text{SSR}}{\\text{SST}} = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n",
    "\\end{split}\n",
    "\\label{eq:1} \\tag{1}\n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Also known as ordinary least squares (OLS) regression. When two variables $X$ (dependent) and $Y$ (independent) are linearly related, their relationship can be written in the form of a regression line:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{Y} = a + BX\n",
    "\\label{eq:2} \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "where $a$ and $B$ are constants. Constant $B$ is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "B = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = \\rho \\frac{\\sigma_y}{\\sigma_x} \n",
    "\\label{eq:3} \\tag{3}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\rho$, the Pearson correlation coefficient, is equal to: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\rho = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}\n",
    "\\label{eq:4} \\tag{4}\n",
    "\\end{equation*}\n",
    "\n",
    "The second constant, $a$, is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "a = \\frac{\\sum y_i}{n} - B \\frac{\\sum x_i}{n} = \\bar{y} - B \\bar{x}\n",
    "\\label{eq:5} \\tag{5}\n",
    "\\end{equation*}\n",
    "\n",
    "### Derivation\n",
    "\n",
    "Let's first revisit what we know. We're given $n$ inputs and outputs $(x_i, y_i)$, so we can define our line of best fit as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y_i} = a + B x_i\n",
    "\\label{eq:6} \\tag{6}\n",
    "\\end{equation*}\n",
    "\n",
    "such that it minimizes the following cost function (RSS):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "S &= \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\\\\n",
    "&= \\sum_{i=1}^{n} (y_i - a - B x_i)^2\n",
    "\\end{split}\n",
    "\\label{eq:7} \\tag{7}\n",
    "\\end{equation*}\n",
    "\n",
    "Now, we want to solve for constants $a$ and $B$. To do this, we should minimize $S$ by setting its partial derivative with respect to both $a$ and $B$ to zero. Let's start with $a$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\frac{\\delta S}{\\delta a} \\sum_{i=1}^{n} (y_i - a - B x_i)^2 = 0 \\\\\n",
    "&\\Rightarrow -2 \\sum_{i=1}^{n} (y_i - a - B x_i) = 0\n",
    "\\end{split} \n",
    "\\label{eq:8} \\tag{8}\n",
    "\\end{equation*}\n",
    "\n",
    "Knowing that $\\sum_{i=1}^{n} a = na$ and after expanding the above, we get:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\Rightarrow \\sum_{i=1}^{n} y_i - na - B \\sum_{i=1}^{n} x_i = 0 \\\\\n",
    "&\\Rightarrow a = \\frac{\\sum_{i=1}^{n} y_i - B \\sum_{i=1}^{n} x_i}{n} \\\\\n",
    "&\\Rightarrow \\boxed{a = \\bar{y} - B \\bar{x}}\n",
    "\\end{split} \n",
    "\\label{eq:9} \\tag{9}\n",
    "\\end{equation*}\n",
    "\n",
    "Now solving for $B$, we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\frac{\\delta S}{\\delta B} \\sum_{i=1}^{n} (y_i - a - B x_i)^2 = 0 \\\\\n",
    "&\\Rightarrow 2 \\sum_{i=1}^{n} -x_i (y_i - a - B x_i) = 0 \\\\ \n",
    "\\end{split}\n",
    "\\label{eq:10} \\tag{10}\n",
    "\\end{equation*}\n",
    "\n",
    "which, when combined with our definition of $a$ above, gives:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&\\sum_{i=1}^{n} (\\bar{y} x_i - x_i y_i - B \\bar{x} x_i + B x_i^2) = 0 \\\\\n",
    "\\Rightarrow B &= \\frac{\\sum_{i=1}^{n} x_i(y_i - \\bar{y})}{\\sum_{i=1}^{n} x_i (x_i - \\bar{x})} \\\\\n",
    "&= \\boxed{\\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}}\n",
    "\\end{split}\n",
    "\\label{eq:11} \\tag{11}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed-Form Solution\n",
    "\n",
    "We can find the closed-form solution of linear regression by minimizing its loss function. The squared error loss is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "L = \\frac{1}{2} \\sum_{i=1}^{m} (y_{i, \\text{pred}} - y_i)^2\n",
    "\\label{eq:12} \\tag{12}\n",
    "\\end{equation*}\n",
    "\n",
    "where $y_{i,\\text{pred}} = h_\\theta (x) = \\sum_{i=0}^{m} \\theta_i x_{j,i} = \\theta^T X$ and $i$ is the number of features for a given $j$. $x$ here is an $m \\times n$ matrix, $y$ is an $m \\times 1$ vector, and $\\theta$ is an $n \\times 1$ vector. The matrix notation of $y_{i,\\text{pred}}$ is:\n",
    "\n",
    "$$\n",
    "Y_{\\text{pred}} =\n",
    "\\begin{bmatrix}\n",
    "    y_{1,\\text{pred}} \\\\\n",
    "    y_{2,\\text{pred}} \\\\\n",
    "    \\vdots \\\\\n",
    "    y_{m, \\text{pred}}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    \\theta_0 + \\theta_1 x_{1,1} + \\cdots + \\theta_1 x_{1,n} \\\\\n",
    "    \\theta_0 + \\theta_1 x_{2,1} + \\cdots + \\theta_1 x_{2,n} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_0 + \\theta_1 x_{m,1} + \\cdots + \\theta_1 x_{m,n} \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    x_{1,0} & x_{1,1} & \\cdots & x_{1,n} \\\\\n",
    "    x_{2,0} & x_{2,1} & \\cdots & x_{2,n} \\\\\n",
    "    &\\vdots \\\\\n",
    "    x_{m,0} & x_{m,1} & \\cdots & x_{m,n} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\theta_0 \\\\\n",
    "    \\theta_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_m \\\\\n",
    "\\end{bmatrix}\n",
    "= X\\theta\n",
    "\\label{eq:13} \\tag{13}\n",
    "$$\n",
    "\n",
    "So the matrix notation of the loss function is:\n",
    "\n",
    "\\begin{equation*}\n",
    "L(\\theta) = \\frac{1}{2} (X\\theta - Y)^2\n",
    "\\label{eq:14} \\tag{14}\n",
    "\\end{equation*}\n",
    "\n",
    "Knowing that the square of a vector is a scalar, i.e. $X^2 = X^T X$, and that $(AB)^T = B^TA^T$ for some $A$ and $B$, we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "L(\\theta) &= (X\\theta - Y)^2 \\\\\n",
    "&= (X\\theta-Y)^T(X\\theta-Y) \\\\\n",
    "&= (\\theta^T X^T - Y^T)(X\\theta - Y) \\\\\n",
    "&= \\theta^T X^T X \\theta - \\theta^T X^T Y - Y^T X \\theta + Y^T Y\n",
    "\\end{split}\n",
    "\\label{eq:15} \\tag{15}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "\n",
    "To minimize the loss function, we set its derivative equal to 0. First, let's find the derivative of $L(\\theta)$. Knowing that $\\frac{d}{d\\theta} A^T\\theta = A$, where $A \\in \\mathbb{R}^n$, and $\\frac{d}{d\\theta} \\theta^T B \\theta = 2B \\theta$, where $B \\in \\mathbb{R}^{mxn}$, we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "\\frac{dL(\\theta)}{d\\theta} &= 2 X^T X \\theta - \\frac{d}{d\\theta}(\\theta^T X^T Y + \\theta^T X^T Y) - 0 \\\\\n",
    "&= 2 X^T X \\theta - \\frac{d}{d\\theta}(2\\theta^T X^T Y) \\\\\n",
    "&= 2 X^T X \\theta - 2 X^T Y\n",
    "\\end{split}\n",
    "\\label{eq:16} \\tag{16}\n",
    "\\end{equation*}\n",
    "\n",
    "**Note:** When solving for $\\frac{d}{d\\theta}Y^T X \\theta$ above, we notice that $Y^T X \\theta$ is a scalar since $(1 \\times m) \\times (m \\times n) \\times (n \\times 1) = (1 \\times 1)$. The transpose of a scalar is still a scalar (i.e. $A^T = A$, where $A \\in \\mathbb{R}$), so we can rewrite $Y^T X \\theta = (Y^T X \\theta)^T = \\theta^T X^T Y$.\n",
    "\n",
    "Now, by setting $\\frac{dL(\\theta)}{d\\theta} = 0$, we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "&X^T X \\theta = X^T Y \\\\\n",
    "&\\Rightarrow (X^T X)^{-1} X^T X \\theta = (X^T X)^{-1} X^T Y \\\\\n",
    "&\\Rightarrow \\boxed{\\theta = (X^T X)^{-1} X^T Y}\n",
    "\\end{split}\n",
    "\\label{eq:17} \\tag{17}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression is similar to simple linear regression except instead of one independent variable, there are multiple. When $Y$ has a linear dependency on $m$ variables $x_1, x_2,..., x_m$:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{Y} = a + b_1 x_1 + b_2 x_2 + ... + b_m x_m\n",
    "\\label{eq:18} \\tag{18}\n",
    "\\end{equation*}\n",
    "\n",
    "## LASSO Regression\n",
    "\n",
    "Linear regression models can be subject to overfitting. LASSO regression, also known as **L1 regularization** (a type of feature selection in machine learning), aims to fix this by eliminating features from the model. It does this by setting feature coefficients to zero using an L1 penalty equal to the absolute value of the magnitude of the coefficients. It particularly works well in cases where there are a small number of significant parameters and the others have coefficients close to zero. \n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Also known as **L2 regularization**. It's similar to LASSO regression except, instead of eliminating features entirely, their coefficients are minimized to some number close to zero by adding an L2 penalty equal to the square of the magnitude of coefficients. Since its not getting rid of any features--only minimizing their effect on the model--ridge regression isn't considered part of feature selection. It works particularly well when there are many significant parameters with close to the same value. \n",
    "\n",
    "## Stepwise Regression\n",
    "\n",
    "Starts with the simplest model, evaluates its performance, then adds another variable, evaluates its performance, then compares it to the previous model, etc. until the best performing model is found."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
