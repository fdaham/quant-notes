{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is an important technique used to avoid overfitting linear models. This includes cases where there are more parameters than observations or the model being used is too complex. However, regularization can also be useful in cases where the number of observations is greater than the number of parameters. For example, without regularization, if all hyperparameters in your model were held constant, more observations would lead to more steps along the gradient at the same learning rate--which leads to overfitting.\n",
    "\n",
    "## LASSO Regression\n",
    "\n",
    "LASSO (least absolute shrinkage and selection operator) regression, also known as **L1 regularization**, is a type of feature selection used in machine learning and works particularly well in cases where there are a small number of significant parameters and the others have coefficients closer to zero. It aims to fix linear regression models that are subject to overfitting by eliminating features from the model. This is done by setting feature coefficients to zero using an L1 penalty equal to the absolute value of the magnitude of the coefficients: \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^m (y_i - \\sum_{j=1}^{n} x_{i,j}B_j)^2 + \\lambda \\sum_{j=1}^{n} |B_j|\n",
    "\\label{eq:1} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\sum_{i=1}^m (y_i - \\sum_{j=1}^{n} x_{i,j}B_j)^2$ is the residual sum of squares, $\\lambda$ is a hyperparameter used to tune the shrinkage value, and $\\sum_{j=1}^{n} |B_j|$ is the absolute value of the magnitude of the regression coefficient. All features are considered when $\\lambda = 0$ and none are considered when $\\lambda = \\infty$. The bias increases as $\\lambda$ increases. When $\\lambda$ decreases, the variance increases. You can use cross validation to find the right value for $\\lambda$.\n",
    "\n",
    "This is how to initialize the LASSO regressor with a cross validated $\\lambda$ using `sklearn`. A few things to note: \n",
    "* `LassoCV()` by default uses LOOCV (leave one out cross validation). \n",
    "* `alpha` is used instead of $\\lambda$.\n",
    "* `Lasso()` by default doesn't normalize regressors, which we need in order to get our values between between 0 and 1. Setting `normalize=True` will normalize the regressors by subtracting the mean and dividing by the l2-norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "# Find lambda\n",
    "lasso_cv = LassoCV(scoring='neg_mean_squared_error', normalize=True)\n",
    "lasso_cv.fit(x_train, y_train)\n",
    "a = lasso_cv.alpha_\n",
    "\n",
    "# Initialize LASSO regressor\n",
    "lasso_regressor = Lasso(alpha=a, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge regression, also known as **L2 regularization**, is similar to LASSO regression but, instead of eliminating features entirely, their coefficients are minimized to some number close to zero. This is done by adding an L2 penalty equal to the square of the magnitude of coefficients. Since its not getting rid of any features--only minimizing their effect on the model--ridge regression isn't considered part of feature selection and is therefore not particularly useful when you have millions of features in your model. Instead, it works well when there are a lesser amount of many significant parameters with close to the same value. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{i=1}^m (y_i - \\sum_{j=1}^{n} x_{i,j}B_j)^2 + \\lambda \\sum_{j=1}^{n} B_j^2\n",
    "\\label{eq:2} \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "This is how you initialize the ridge regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "# Find lambda\n",
    "ridge_cv = RidgeCV(scoring='neg_mean_squared_error', normalize=True)\n",
    "ridge_cv.fit(x_train, y_train)\n",
    "a = ridge_cv.alpha_\n",
    "\n",
    "# Initialize ridge regressor\n",
    "ridge_regressor = Ridge(alpha=a, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Regression\n",
    "\n",
    "Stepwise regression is an iterative linear regression technique that begins with the simplest model, evaluates its performance, compares it to the previous model, then adds or removes a variable until the best model is found. It can be a very compute heavy operation, so Ridge and LASSO regression (which both require much less parameter tuning) are often used as alternatives. Stepwise regression is a combination of forward and backward regression since it alternates between the two selection techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
